{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>DDataFlow is an end2end tests and local development solution for machine learning and data pipelines using pyspark.</p> <p>It allows you to: - Read a subset of our data so to speed up the running of the pipelines during tests - Write to a test location our artifacts so you don't pollute production - Download data for enabling local machine development</p> <p>Below is the DDataFlow integration manual. If you want to know how to use DDataFlow in the local machine, jump to this section.</p>"},{"location":"#install-ddataflow","title":"Install Ddataflow","text":"<pre><code>pip install ddataflow\n</code></pre>"},{"location":"#mapping-your-data-sources","title":"Mapping your data sources","text":"<p>DDataflow is declarative and is completely configurable a single configuration in DDataflow startup. To create a configuration for you project simply run:</p> <pre><code>ddataflow setup_project\n</code></pre> <p>You can use this config also in in a notebook, or using databricks-connect or in the repository with db-rocket. Example config below:</p> <pre><code>#later save this script as ddataflow_config.py to follow our convention\nfrom ddataflow import DDataflow\nimport pyspark.sql.functions as F\n\nstart_time = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\nend_time = datetime.now().strftime(\"%Y-%m-%d\")\n\nconfig = {\n    \"data_sources\": {\n        # data sources define how to access data\n        \"events\": {\n            \"source\": lambda spark: spark.table(\"events\"),\n            #  here we define the spark query to reduce the size of the data\n            #  the filtering strategy will most likely dependend on the domain.\n            \"filter\": lambda df:\n                df.filter(F.col(\"date\") &gt;= start_time)\n                    .filter(F.col(\"date\") &lt;= end_time)\n                    .filter(F.col(\"event_name\").isin([\"BookAction\", \"ActivityCardImpression\"])),\n        },\n        \"ActivityCardImpression\": {\n            # source can also be partquet files\n            \"source\": lambda spark: spark.read.parquet(\n                f\"dbfs:/events/eventname/date={start_time}/\"\n            )\n        },\n    },\n    \"project_folder_name\": \"myproject\",\n}\n\n# initialize the application and validate the configuration\nddataflow_client = DDataflow(**config)\n</code></pre>"},{"location":"#replace-the-sources","title":"Replace the sources","text":"<p>Replace in your code the calls to the original data sources for the ones provided by ddataflow.</p> <pre><code>spark.table('events') #...\nspark.read.parquet(\"dbfs:/mnt/analytics/cleaned/v1/ActivityCardImpression\") # ...\n</code></pre> <p>Replace with the following:</p> <pre><code>from ddataflow_config import ddataflow_client\n\nddataflow_client.source('events')\nddataflow_client.source(\"ActivityCardImpression\")\n</code></pre> <p>Its not a problem if you dont map all data sources if you dont map one it will keep going to production tables and might be slower. From this point you can use dddataflow to run your pipelines on the sample data instead of the full data.</p> <p>Note: BY DEFAULT ddataflow is DISABLED, so the calls will attempt to go to production, which if done wrong can lead to writing trash data.</p> <p>To enable DDataFlow you can either export an environment variable without changing the code.</p> <pre><code># in shell or in the CICD pipeline\nexport ENABLE_DDATAFLOW=true\n# run your pipeline as normal\npython conduction_time_predictor/train.py\n</code></pre> <p>Or you can enable it programmatically in python</p> <pre><code>ddataflow_client.enable()\n</code></pre> <p>At any point in time you can check if the tool is enabled or disabled by running:</p> <pre><code>ddataflow_client.print_status()\n</code></pre>"},{"location":"#writing-data","title":"Writing data","text":"<p>To write data we adivse you use the same code as production just write to a different destination. DDataflow provides the path function that will return a staging path when ddataflow is enabled.</p> <pre><code>final_path = ddataflow.path('/mnt/my/production/path')\n# final_path=/mnt/my/production/path when ddataflow is DISABLED\n# final path=$DDATAFLOW_FOLDER/project_name/mnt/my/production/path when ddataflow is ENABLED\n</code></pre> <p>And you are good to go!</p>"},{"location":"FAQ/","title":"FAQ","text":""},{"location":"FAQ/#i-am-trying-to-download-data-but-the-system-is-complaining-my-databricks-cli-is-not-configure","title":"I am trying to download data but the system is complaining my databricks cli is not configure","text":"<p>After installing ddataflow run the configure producedure in your installed machine</p> <pre><code>databricks configure --token\n</code></pre> <p>Follow the wizard until the end.</p>"},{"location":"local_development/","title":"Local Development","text":"<p>DDataflow also enables one to develop with local data. We see this though as a more advanced use case, which might be the first choice for everybody. First, make a copy of the files you need to download in dbfs.</p> <pre><code>ddataflow.save_sampled_data_sources(ask_confirmation=False)\n</code></pre> <p>Then in your machine:</p> <pre><code>ddataflow current_project download_data_sources\n</code></pre> <p>Now you can use the pipeline locally by exporting the following env variables:</p> <pre><code>export ENABLE_OFFLINE_MODE=true\n# run your pipeline as normal\npython yourproject/train.py\n</code></pre> <p>The downloaded data sources will be stored at <code>$HOME/.ddataflow</code>.</p>"},{"location":"local_development/#local-setup-for-spark","title":"Local setup for spark","text":"<p>if you run spark locally you might need to tweak some parameters compared to your cluster. Below is a good example you can use.</p> <pre><code>def configure_spark():\n\n    if ddataflow.is_local():\n        import pyspark\n\n        spark_conf = pyspark.SparkConf()\n        spark_conf.set(\"spark.sql.warehouse.dir\", \"/tmp\")\n        spark_conf.set(\"spark.sql.catalogImplementation\", \"hive\")\n        spark_conf.set(\"spark.driver.memory\", \"15g\")\n        spark_conf.setMaster(\"local[*]\")\n        sc = pyspark.SparkContext(conf=spark_conf)\n        session = pyspark.sql.SparkSession(sc)\n\n        return session\n\n    return SparkSession.builder.getOrCreate()\n</code></pre> <p>If you run into Snappy compression problem: Please reinstall pyspark! </p>"},{"location":"sampling/","title":"Sampling","text":""},{"location":"sampling/#sampling-on-the-notebook","title":"Sampling on the notebook","text":"<p>Add the following to your setup.py</p> <pre><code>    # given that ddataflow config usually sits on the root of the project\n    # we add it to the package data manually if we want to access the config \n    # installed as a library\n    py_modules=[\n        \"ddataflow_config\",\n    ],\n</code></pre>"},{"location":"sampling/#with-dbrocket","title":"With DBrocket","text":"<p>Cell 1</p> <pre><code>%pip install --upgrade pip \n%pip install ddataflow\n%pip install /dbfs/temp/user/search_ranking_pipeline-1.0.1-py3-none-any.whl --force-reinstall`\n</code></pre> <p>Cell 2</p> <pre><code>from ddataflow_config import ddataflow\nddataflow.save_sampled_data_sources()\n</code></pre> <p>Then use dry_run=False when you are ready to copy.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>One drawback of having ddataflow in the root folder is that it can conflict with other ddtaflow installations. Prefer installing ddataflow in submodules of your main project (<code>myproject/main_module/ddataflow_config.py</code>) instead of globally (<code>myproject/ddataflow_config.py</code>).</p>"},{"location":"api_reference/DDataflow/","title":"DDataflow","text":"<p>DDataflow is an end2end tests solution. See our docs manual for more details. Additionally, use help(ddataflow) to see the available methods.</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>class DDataflow:\n    \"\"\"\n    DDataflow is an end2end tests solution.\n    See our docs manual for more details.\n    Additionally, use help(ddataflow) to see the available methods.\n    \"\"\"\n\n    _DEFAULT_SNAPSHOT_BASE_PATH = \"dbfs:/ddataflow\"\n    _LOCAL_BASE_SNAPSHOT_PATH = os.environ[\"HOME\"] + \"/.ddataflow\"\n    _ENABLE_DDATAFLOW_ENVVARIABLE = \"ENABLE_DDATAFLOW\"\n    _ENABLE_OFFLINE_MODE_ENVVARIABLE = \"ENABLE_OFFLINE_MODE\"\n    _DDATAFLOW_CONFIG_FILE = \"ddataflow_config.py\"\n\n    _local_path: str\n\n    def __init__(\n        self,\n        project_folder_name: str,\n        data_sources: Optional[dict] = None,\n        data_writers: Optional[dict] = None,\n        data_source_size_limit_gb: int = 1,\n        enable_ddataflow=False,\n        sources_with_default_sampling: Optional[List[str]] = None,\n        snapshot_path: Optional[str] = None,\n        default_sampler: Optional[dict] = None,\n        default_database: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize the dataflow object.\n        The input of this object is the config dictionary outlined in our integrator manual.\n\n        Important params:\n        project_folder_name:\n            the name of the project that will be stored in the disk\n        snapshot_path:\n            path to the snapshot folder\n        data_source_size_limit_gb:\n            limit the size of the data sources\n        default_sampler:\n         options to pass to the default sampler\n        sources_with_default_sampling:\n         if you have tables you want to have by default and dont want to sample them first\n        default_database:\n            name of the default database. If ddataflow is enabled, a test db will be created and used.\n        sources_with_default_sampling :\n         Deprecated: use sources with default_sampling=True instead\n         if you have tables you want to have by default and dont want to sample them first\n        \"\"\"\n        self._size_limit = data_source_size_limit_gb\n\n        self.project_folder_name = project_folder_name\n\n        base_path = snapshot_path if snapshot_path else self._DEFAULT_SNAPSHOT_BASE_PATH\n\n        self._snapshot_path = base_path + \"/\" + project_folder_name\n        self._local_path = self._LOCAL_BASE_SNAPSHOT_PATH + \"/\" + project_folder_name\n\n        if default_sampler:\n            # set this before creating data sources\n            DefaultSamplerOptions.set(default_sampler)\n\n        if not data_sources:\n            data_sources = {}\n\n        all_data_sources = {\n            **build_default_sampling_for_sources(sources_with_default_sampling),\n            **data_sources,\n        }\n\n        self._data_sources = DataSources(\n            config=all_data_sources,\n            local_folder=self._local_path,\n            snapshot_path=self._snapshot_path,\n            size_limit=self._size_limit,\n        )\n\n        self._data_writers: dict = data_writers if data_writers else {}\n\n        self._offline_enabled = os.getenv(self._ENABLE_OFFLINE_MODE_ENVVARIABLE, False)\n\n        self._ddataflow_enabled: Union[str, bool] = os.getenv(\n            self._ENABLE_DDATAFLOW_ENVVARIABLE, enable_ddataflow\n        )\n\n        # if offline is enabled we should use local data\n        if self._offline_enabled:\n            self.enable_offline()\n\n        self.save_sampled_data_sources = Sampler(\n            self._snapshot_path, self._data_sources\n        ).save_sampled_data_sources\n\n        if default_database:\n            self.set_up_database(default_database)\n\n        # Print detailed logs when ddataflow is enabled\n        if self._ddataflow_enabled:\n            self.set_logger_level(logging.DEBUG)\n        else:\n            logger.info(\n                \"DDataflow is now DISABLED.\"\n                \"PRODUCTION data will be used and it will write to production tables.\"\n            )\n\n    @staticmethod\n    def setup_project():\n        \"\"\"\n        Sets up a new ddataflow project with empty data sources in the current directory\n        \"\"\"\n        from ddataflow.setup.setup_project import setup_project\n\n        setup_project()\n\n    @staticmethod\n    def current_project() -&gt; \"DDataflow\":\n        \"\"\"\n        Returns a ddataflow configured with the current directory configuration file\n        Requirements for this to work:\n\n        1. MLTools must be called from withing the project root directory\n        2. There must be a file called ddataflow_config.py there\n        3. the module must have defined DDataflow object with the name of ddataflow\n\n        @todo investigate if we can use import_class_from_string\n        \"\"\"\n        import sys\n\n        CONFIGURATION_FILE_NAME = \"ddataflow_config.py\"\n\n        current_folder = os.getcwd()\n        logger.debug(\"Loading config from folder\", current_folder)\n        config_location = os.path.join(current_folder, CONFIGURATION_FILE_NAME)\n\n        if not os.path.exists(config_location):\n            raise Exception(\n                f\"\"\"\nThis command needs to be executed within a project containing a {CONFIGURATION_FILE_NAME} file.\nYou can start a new one for the current folder by running the following command:\n$ ddataflow setup_project\"\"\"\n            )\n\n        sys.path.append(current_folder)\n\n        import ddataflow_config\n\n        if hasattr(ddataflow_config, \"ddataflow_client\"):\n            return ddataflow_config.ddataflow_client\n\n        if not hasattr(ddataflow_config, \"ddataflow\"):\n            raise Exception(\"ddataflow object is not defined in your _config file\")\n\n        return ddataflow_config.ddataflow\n\n    def source(self, name: str, debugger=False) -&gt; DataFrame:\n        \"\"\"\n        Gives access to the data source configured in the dataflow\n\n        You can also use this function in the terminal with --debugger=True to inspect the dataframe.\n        \"\"\"\n        self.print_status()\n\n        logger.debug(\"Loading data source\")\n        data_source: DataSource = self._data_sources.get_data_source(name)\n        logger.debug(\"Data source loaded\")\n        df = self._get_data_from_data_source(data_source)\n\n        if debugger:\n            logger.debug(f\"Debugger enabled: {debugger}\")\n            logger.debug(\"In debug mode now, use query to inspect it\")\n            breakpoint()\n\n        return df\n\n    def _get_spark(self):\n        return get_or_create_spark()\n\n    def enable(self):\n        \"\"\"\n        When enabled ddataflow will read from the filtered data sources\n        instead of production tables. And write to testing tables instead of production ones.\n        \"\"\"\n\n        self._ddataflow_enabled = True\n\n    def is_enabled(self) -&gt; bool:\n        return self._ddataflow_enabled\n\n    def enable_offline(self):\n        \"\"\"Programatically enable offline mode\"\"\"\n        self._offline_enabled = True\n        self.enable()\n\n    def is_local(self) -&gt; bool:\n        return self._offline_enabled\n\n    def disable_offline(self):\n        \"\"\"Programatically enable offline mode\"\"\"\n        self._offline_enabled = False\n\n    def source_name(self, name, disable_view_creation=False) -&gt; str:\n        \"\"\"\n        Given the name of a production table, returns the name of the corresponding ddataflow table when ddataflow is enabled\n        If ddataflow is disabled get the production one.\n        \"\"\"\n        logger.debug(\"Source name used: \", name)\n        source_name = name\n\n        # the gist of ddtafalow\n        if self._ddataflow_enabled:\n            source_name = self._get_new_table_name(name)\n            if disable_view_creation:\n                return source_name\n\n            logger.debug(f\"Creating a temp view with the name: {source_name}\")\n            data_source: DataSource = self._data_sources.get_data_source(name)\n\n            if self._offline_enabled:\n                df = data_source.query_locally()\n            else:\n                df = data_source.query()\n\n            df.createOrReplaceTempView(source_name)\n\n            return source_name\n\n        return source_name\n\n    def path(self, path):\n        \"\"\"\n        returns a deterministic path replacing the real production path with one based on the current environment needs.\n        Currently support path starts with 'dbfs:/' and 's3://'.\n        \"\"\"\n        if not self._ddataflow_enabled:\n            return path\n\n        base_path = self._get_current_environment_data_folder()\n\n        for path_prefix in [\"dbfs:/\", \"s3://\"]:\n            path = path.replace(path_prefix, \"\")\n\n        return base_path + \"/\" + path\n\n    def set_up_database(self, db_name: str):\n        \"\"\"\n        Perform USE $DATABASE query to set up a default database.\n        If ddataflow is enabled, use a test db to prevent writing data into production.\n        \"\"\"\n        # rename database if ddataflow is enabled\n        if self._ddataflow_enabled:\n            db_name = f\"ddataflow_{db_name}\"\n        # get spark\n        spark = self._get_spark()\n        # create db if not exist\n        sql = \"CREATE DATABASE IF NOT EXISTS {0}\".format(db_name)\n        spark.sql(sql)\n        # set default db\n        spark.sql(\"USE {}\".format(db_name))\n        logger.warning(f\"The default database is now set to {db_name}\")\n\n    def _get_new_table_name(self, name) -&gt; str:\n        overriden_name = name.replace(\"dwh.\", \"\")\n        return self.project_folder_name + \"_\" + overriden_name\n\n    def name(self, *args, **kwargs):\n        \"\"\"\n        A shorthand for source_name\n        \"\"\"\n        return self.source_name(*args, **kwargs)\n\n    def disable(self):\n        \"\"\"Disable ddtaflow overriding tables, uses production state in other words\"\"\"\n        self._ddataflow_enabled = False\n\n    def _get_data_from_data_source(self, data_source: DataSource) -&gt; DataFrame:\n        if not self._ddataflow_enabled:\n            logger.debug(\"DDataflow not enabled\")\n            # goes directly to production without prefilters\n            return data_source.query_without_filter()\n\n        if self._offline_enabled:\n            # uses snapshot data\n            if using_databricks_connect():\n                logger.debug(\n                    \"Looks like you are using databricks-connect in offline mode. You probably want to run it \"\n                    \"without databricks connect in offline mode\"\n                )\n\n            return data_source.query_locally()\n\n        logger.debug(\"DDataflow enabled and filtering\")\n        return data_source.query()\n\n    def download_data_sources(self, overwrite: bool = True, debug=False):\n        \"\"\"\n        Download the data sources locally for development offline\n        Note: you need databricks-cli for this command to work\n\n        Options:\n            overwrite: will first clean the existing files\n        \"\"\"\n        DataSourceDownloader().download_all(self._data_sources, overwrite, debug)\n\n    def sample_and_download(\n        self, ask_confirmation: bool = True, overwrite: bool = True\n    ):\n        \"\"\"\n        Create a sample folder in dbfs and then downloads it in the local machine\n        \"\"\"\n        self.save_sampled_data_sources(dry_run=False, ask_confirmation=ask_confirmation)\n        self.download_data_sources(overwrite)\n\n    def write(self, df, name: str):\n        \"\"\"\n        Write a dataframe either to a local folder or the production one\n        \"\"\"\n        if name not in self._data_writers:\n            raise WriterNotFoundException(name)\n\n        if self._ddataflow_enabled:\n            writing_path = self._snapshot_path\n\n            if self._offline_enabled:\n                writing_path = self._local_path\n            else:\n                if not writing_path.startswith(DDataflow._DEFAULT_SNAPSHOT_BASE_PATH):\n                    raise Exception(\n                        f\"Only writing to {DDataflow._DEFAULT_SNAPSHOT_BASE_PATH} is enabled\"\n                    )\n\n            writing_path = os.path.join(writing_path, name)\n            logger.info(\"Writing data to parquet file: \" + writing_path)\n            return df.write.parquet(writing_path, mode=\"overwrite\")\n\n        # if none of the above writes to production\n        return self._data_writers[name][\"writer\"](df, name, self._get_spark())  # type: ignore\n\n    def read(self, name: str):\n        \"\"\"\n        Read the data writers parquet file which are stored in the ddataflow folder\n        \"\"\"\n        path = self._snapshot_path\n        if self._offline_enabled:\n            path = self._local_path\n\n        parquet_path = os.path.join(path, name)\n        return self._get_spark().read.parquet(parquet_path)\n\n    def _print_snapshot_size(self):\n        \"\"\"\n        Prints the final size of the dataset in the folder\n        Note: Only works in notebooks.\n        \"\"\"\n        import subprocess\n\n        location = \"/dbfs/ddataflow/\"\n        output = subprocess.getoutput(f\"du -h -d2 {location}\")\n        print(output)\n\n    def _print_download_folder_size(self):\n        \"\"\"\n        Prints the final size of the dataset in the folder\n        \"\"\"\n        import subprocess\n\n        output = subprocess.getoutput(f\"du -h -d2 {self._local_path}\")\n        print(output)\n\n    def get_mlflow_path(self, original_path: str):\n        \"\"\"\n        overrides the mlflow path if\n        \"\"\"\n        overriden_path = self._get_overriden_arctifacts_current_path()\n        if overriden_path:\n            model_name = original_path.split(\"/\")[-1]\n            return overriden_path + \"/\" + model_name\n\n        return original_path\n\n    def _get_overriden_arctifacts_current_path(self):\n        if self._offline_enabled:\n            return self._local_path\n\n        if self._ddataflow_enabled:\n            return self._snapshot_path\n\n        return None\n\n    def is_enabled(self):\n        \"\"\"\n        To be enabled ddataflow has to be either in offline mode or with enable=True\n        \"\"\"\n        return self._offline_enabled or self._ddataflow_enabled\n\n    def print_status(self):\n        \"\"\"\n        Print the status of the ddataflow\n        \"\"\"\n        if self._offline_enabled:\n            logger.debug(\"DDataflow is now ENABLED in OFFLINE mode\")\n            logger.debug(\n                \"To disable it remove from your code or unset the enviroment variable 'unset ENABLE_DDATAFLOW ; unset ENABLE_OFFLINE_MODE'\"\n            )\n        elif self._ddataflow_enabled:\n            logger.debug(\n                \"\"\"\n                DDataflow is now ENABLED in ONLINE mode. Filtered data will be used and it will write to temporary tables.\n                \"\"\"\n            )\n        else:\n            logger.debug(\n                f\"\"\"\n                DDataflow is now DISABLED. So PRODUCTION data will be used and it will write to production tables.\n                Use enable() function or export {self._ENABLE_DDATAFLOW_ENVVARIABLE}=True to enable it.\n                If you are working offline use export ENABLE_OFFLINE_MODE=True instead.\n                \"\"\"\n            )\n\n    def _get_current_environment_data_folder(self) -&gt; Optional[str]:\n        if not self._ddataflow_enabled:\n            raise Exception(\"DDataflow is disabled so no data folder is available\")\n\n        if self._offline_enabled:\n            return self._local_path\n\n        return self._snapshot_path\n\n    def set_logger_level(self, level):\n        \"\"\"\n        Set logger level.\n        Levels can be found here: https://docs.python.org/3/library/logging.html#logging-levels\n        \"\"\"\n        logger.info(f\"Set logger level to: {level}\")\n        logger.setLevel(level)\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.__init__","title":"<code>__init__(project_folder_name, data_sources=None, data_writers=None, data_source_size_limit_gb=1, enable_ddataflow=False, sources_with_default_sampling=None, snapshot_path=None, default_sampler=None, default_database=None)</code>","text":"<p>Initialize the dataflow object. The input of this object is the config dictionary outlined in our integrator manual.</p> <p>Important params: project_folder_name:     the name of the project that will be stored in the disk snapshot_path:     path to the snapshot folder data_source_size_limit_gb:     limit the size of the data sources default_sampler:  options to pass to the default sampler sources_with_default_sampling:  if you have tables you want to have by default and dont want to sample them first default_database:     name of the default database. If ddataflow is enabled, a test db will be created and used. sources_with_default_sampling :  Deprecated: use sources with default_sampling=True instead  if you have tables you want to have by default and dont want to sample them first</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def __init__(\n    self,\n    project_folder_name: str,\n    data_sources: Optional[dict] = None,\n    data_writers: Optional[dict] = None,\n    data_source_size_limit_gb: int = 1,\n    enable_ddataflow=False,\n    sources_with_default_sampling: Optional[List[str]] = None,\n    snapshot_path: Optional[str] = None,\n    default_sampler: Optional[dict] = None,\n    default_database: Optional[str] = None,\n):\n    \"\"\"\n    Initialize the dataflow object.\n    The input of this object is the config dictionary outlined in our integrator manual.\n\n    Important params:\n    project_folder_name:\n        the name of the project that will be stored in the disk\n    snapshot_path:\n        path to the snapshot folder\n    data_source_size_limit_gb:\n        limit the size of the data sources\n    default_sampler:\n     options to pass to the default sampler\n    sources_with_default_sampling:\n     if you have tables you want to have by default and dont want to sample them first\n    default_database:\n        name of the default database. If ddataflow is enabled, a test db will be created and used.\n    sources_with_default_sampling :\n     Deprecated: use sources with default_sampling=True instead\n     if you have tables you want to have by default and dont want to sample them first\n    \"\"\"\n    self._size_limit = data_source_size_limit_gb\n\n    self.project_folder_name = project_folder_name\n\n    base_path = snapshot_path if snapshot_path else self._DEFAULT_SNAPSHOT_BASE_PATH\n\n    self._snapshot_path = base_path + \"/\" + project_folder_name\n    self._local_path = self._LOCAL_BASE_SNAPSHOT_PATH + \"/\" + project_folder_name\n\n    if default_sampler:\n        # set this before creating data sources\n        DefaultSamplerOptions.set(default_sampler)\n\n    if not data_sources:\n        data_sources = {}\n\n    all_data_sources = {\n        **build_default_sampling_for_sources(sources_with_default_sampling),\n        **data_sources,\n    }\n\n    self._data_sources = DataSources(\n        config=all_data_sources,\n        local_folder=self._local_path,\n        snapshot_path=self._snapshot_path,\n        size_limit=self._size_limit,\n    )\n\n    self._data_writers: dict = data_writers if data_writers else {}\n\n    self._offline_enabled = os.getenv(self._ENABLE_OFFLINE_MODE_ENVVARIABLE, False)\n\n    self._ddataflow_enabled: Union[str, bool] = os.getenv(\n        self._ENABLE_DDATAFLOW_ENVVARIABLE, enable_ddataflow\n    )\n\n    # if offline is enabled we should use local data\n    if self._offline_enabled:\n        self.enable_offline()\n\n    self.save_sampled_data_sources = Sampler(\n        self._snapshot_path, self._data_sources\n    ).save_sampled_data_sources\n\n    if default_database:\n        self.set_up_database(default_database)\n\n    # Print detailed logs when ddataflow is enabled\n    if self._ddataflow_enabled:\n        self.set_logger_level(logging.DEBUG)\n    else:\n        logger.info(\n            \"DDataflow is now DISABLED.\"\n            \"PRODUCTION data will be used and it will write to production tables.\"\n        )\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.current_project","title":"<code>current_project()</code>  <code>staticmethod</code>","text":"<p>Returns a ddataflow configured with the current directory configuration file Requirements for this to work:</p> <ol> <li>MLTools must be called from withing the project root directory</li> <li>There must be a file called ddataflow_config.py there</li> <li>the module must have defined DDataflow object with the name of ddataflow</li> </ol> <p>@todo investigate if we can use import_class_from_string</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>    @staticmethod\n    def current_project() -&gt; \"DDataflow\":\n        \"\"\"\n        Returns a ddataflow configured with the current directory configuration file\n        Requirements for this to work:\n\n        1. MLTools must be called from withing the project root directory\n        2. There must be a file called ddataflow_config.py there\n        3. the module must have defined DDataflow object with the name of ddataflow\n\n        @todo investigate if we can use import_class_from_string\n        \"\"\"\n        import sys\n\n        CONFIGURATION_FILE_NAME = \"ddataflow_config.py\"\n\n        current_folder = os.getcwd()\n        logger.debug(\"Loading config from folder\", current_folder)\n        config_location = os.path.join(current_folder, CONFIGURATION_FILE_NAME)\n\n        if not os.path.exists(config_location):\n            raise Exception(\n                f\"\"\"\nThis command needs to be executed within a project containing a {CONFIGURATION_FILE_NAME} file.\nYou can start a new one for the current folder by running the following command:\n$ ddataflow setup_project\"\"\"\n            )\n\n        sys.path.append(current_folder)\n\n        import ddataflow_config\n\n        if hasattr(ddataflow_config, \"ddataflow_client\"):\n            return ddataflow_config.ddataflow_client\n\n        if not hasattr(ddataflow_config, \"ddataflow\"):\n            raise Exception(\"ddataflow object is not defined in your _config file\")\n\n        return ddataflow_config.ddataflow\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.disable","title":"<code>disable()</code>","text":"<p>Disable ddtaflow overriding tables, uses production state in other words</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def disable(self):\n    \"\"\"Disable ddtaflow overriding tables, uses production state in other words\"\"\"\n    self._ddataflow_enabled = False\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.disable_offline","title":"<code>disable_offline()</code>","text":"<p>Programatically enable offline mode</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def disable_offline(self):\n    \"\"\"Programatically enable offline mode\"\"\"\n    self._offline_enabled = False\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.download_data_sources","title":"<code>download_data_sources(overwrite=True, debug=False)</code>","text":"<p>Download the data sources locally for development offline Note: you need databricks-cli for this command to work</p> <p>Options:     overwrite: will first clean the existing files</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def download_data_sources(self, overwrite: bool = True, debug=False):\n    \"\"\"\n    Download the data sources locally for development offline\n    Note: you need databricks-cli for this command to work\n\n    Options:\n        overwrite: will first clean the existing files\n    \"\"\"\n    DataSourceDownloader().download_all(self._data_sources, overwrite, debug)\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.enable","title":"<code>enable()</code>","text":"<p>When enabled ddataflow will read from the filtered data sources instead of production tables. And write to testing tables instead of production ones.</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def enable(self):\n    \"\"\"\n    When enabled ddataflow will read from the filtered data sources\n    instead of production tables. And write to testing tables instead of production ones.\n    \"\"\"\n\n    self._ddataflow_enabled = True\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.enable_offline","title":"<code>enable_offline()</code>","text":"<p>Programatically enable offline mode</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def enable_offline(self):\n    \"\"\"Programatically enable offline mode\"\"\"\n    self._offline_enabled = True\n    self.enable()\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.get_mlflow_path","title":"<code>get_mlflow_path(original_path)</code>","text":"<p>overrides the mlflow path if</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def get_mlflow_path(self, original_path: str):\n    \"\"\"\n    overrides the mlflow path if\n    \"\"\"\n    overriden_path = self._get_overriden_arctifacts_current_path()\n    if overriden_path:\n        model_name = original_path.split(\"/\")[-1]\n        return overriden_path + \"/\" + model_name\n\n    return original_path\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.is_enabled","title":"<code>is_enabled()</code>","text":"<p>To be enabled ddataflow has to be either in offline mode or with enable=True</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def is_enabled(self):\n    \"\"\"\n    To be enabled ddataflow has to be either in offline mode or with enable=True\n    \"\"\"\n    return self._offline_enabled or self._ddataflow_enabled\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.name","title":"<code>name(*args, **kwargs)</code>","text":"<p>A shorthand for source_name</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def name(self, *args, **kwargs):\n    \"\"\"\n    A shorthand for source_name\n    \"\"\"\n    return self.source_name(*args, **kwargs)\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.path","title":"<code>path(path)</code>","text":"<p>returns a deterministic path replacing the real production path with one based on the current environment needs. Currently support path starts with 'dbfs:/' and 's3://'.</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def path(self, path):\n    \"\"\"\n    returns a deterministic path replacing the real production path with one based on the current environment needs.\n    Currently support path starts with 'dbfs:/' and 's3://'.\n    \"\"\"\n    if not self._ddataflow_enabled:\n        return path\n\n    base_path = self._get_current_environment_data_folder()\n\n    for path_prefix in [\"dbfs:/\", \"s3://\"]:\n        path = path.replace(path_prefix, \"\")\n\n    return base_path + \"/\" + path\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.print_status","title":"<code>print_status()</code>","text":"<p>Print the status of the ddataflow</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def print_status(self):\n    \"\"\"\n    Print the status of the ddataflow\n    \"\"\"\n    if self._offline_enabled:\n        logger.debug(\"DDataflow is now ENABLED in OFFLINE mode\")\n        logger.debug(\n            \"To disable it remove from your code or unset the enviroment variable 'unset ENABLE_DDATAFLOW ; unset ENABLE_OFFLINE_MODE'\"\n        )\n    elif self._ddataflow_enabled:\n        logger.debug(\n            \"\"\"\n            DDataflow is now ENABLED in ONLINE mode. Filtered data will be used and it will write to temporary tables.\n            \"\"\"\n        )\n    else:\n        logger.debug(\n            f\"\"\"\n            DDataflow is now DISABLED. So PRODUCTION data will be used and it will write to production tables.\n            Use enable() function or export {self._ENABLE_DDATAFLOW_ENVVARIABLE}=True to enable it.\n            If you are working offline use export ENABLE_OFFLINE_MODE=True instead.\n            \"\"\"\n        )\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.read","title":"<code>read(name)</code>","text":"<p>Read the data writers parquet file which are stored in the ddataflow folder</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def read(self, name: str):\n    \"\"\"\n    Read the data writers parquet file which are stored in the ddataflow folder\n    \"\"\"\n    path = self._snapshot_path\n    if self._offline_enabled:\n        path = self._local_path\n\n    parquet_path = os.path.join(path, name)\n    return self._get_spark().read.parquet(parquet_path)\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.sample_and_download","title":"<code>sample_and_download(ask_confirmation=True, overwrite=True)</code>","text":"<p>Create a sample folder in dbfs and then downloads it in the local machine</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def sample_and_download(\n    self, ask_confirmation: bool = True, overwrite: bool = True\n):\n    \"\"\"\n    Create a sample folder in dbfs and then downloads it in the local machine\n    \"\"\"\n    self.save_sampled_data_sources(dry_run=False, ask_confirmation=ask_confirmation)\n    self.download_data_sources(overwrite)\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.set_logger_level","title":"<code>set_logger_level(level)</code>","text":"<p>Set logger level. Levels can be found here: https://docs.python.org/3/library/logging.html#logging-levels</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def set_logger_level(self, level):\n    \"\"\"\n    Set logger level.\n    Levels can be found here: https://docs.python.org/3/library/logging.html#logging-levels\n    \"\"\"\n    logger.info(f\"Set logger level to: {level}\")\n    logger.setLevel(level)\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.set_up_database","title":"<code>set_up_database(db_name)</code>","text":"<p>Perform USE $DATABASE query to set up a default database. If ddataflow is enabled, use a test db to prevent writing data into production.</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def set_up_database(self, db_name: str):\n    \"\"\"\n    Perform USE $DATABASE query to set up a default database.\n    If ddataflow is enabled, use a test db to prevent writing data into production.\n    \"\"\"\n    # rename database if ddataflow is enabled\n    if self._ddataflow_enabled:\n        db_name = f\"ddataflow_{db_name}\"\n    # get spark\n    spark = self._get_spark()\n    # create db if not exist\n    sql = \"CREATE DATABASE IF NOT EXISTS {0}\".format(db_name)\n    spark.sql(sql)\n    # set default db\n    spark.sql(\"USE {}\".format(db_name))\n    logger.warning(f\"The default database is now set to {db_name}\")\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.setup_project","title":"<code>setup_project()</code>  <code>staticmethod</code>","text":"<p>Sets up a new ddataflow project with empty data sources in the current directory</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>@staticmethod\ndef setup_project():\n    \"\"\"\n    Sets up a new ddataflow project with empty data sources in the current directory\n    \"\"\"\n    from ddataflow.setup.setup_project import setup_project\n\n    setup_project()\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.source","title":"<code>source(name, debugger=False)</code>","text":"<p>Gives access to the data source configured in the dataflow</p> <p>You can also use this function in the terminal with --debugger=True to inspect the dataframe.</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def source(self, name: str, debugger=False) -&gt; DataFrame:\n    \"\"\"\n    Gives access to the data source configured in the dataflow\n\n    You can also use this function in the terminal with --debugger=True to inspect the dataframe.\n    \"\"\"\n    self.print_status()\n\n    logger.debug(\"Loading data source\")\n    data_source: DataSource = self._data_sources.get_data_source(name)\n    logger.debug(\"Data source loaded\")\n    df = self._get_data_from_data_source(data_source)\n\n    if debugger:\n        logger.debug(f\"Debugger enabled: {debugger}\")\n        logger.debug(\"In debug mode now, use query to inspect it\")\n        breakpoint()\n\n    return df\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.source_name","title":"<code>source_name(name, disable_view_creation=False)</code>","text":"<p>Given the name of a production table, returns the name of the corresponding ddataflow table when ddataflow is enabled If ddataflow is disabled get the production one.</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def source_name(self, name, disable_view_creation=False) -&gt; str:\n    \"\"\"\n    Given the name of a production table, returns the name of the corresponding ddataflow table when ddataflow is enabled\n    If ddataflow is disabled get the production one.\n    \"\"\"\n    logger.debug(\"Source name used: \", name)\n    source_name = name\n\n    # the gist of ddtafalow\n    if self._ddataflow_enabled:\n        source_name = self._get_new_table_name(name)\n        if disable_view_creation:\n            return source_name\n\n        logger.debug(f\"Creating a temp view with the name: {source_name}\")\n        data_source: DataSource = self._data_sources.get_data_source(name)\n\n        if self._offline_enabled:\n            df = data_source.query_locally()\n        else:\n            df = data_source.query()\n\n        df.createOrReplaceTempView(source_name)\n\n        return source_name\n\n    return source_name\n</code></pre>"},{"location":"api_reference/DDataflow/#ddataflow.ddataflow.DDataflow.write","title":"<code>write(df, name)</code>","text":"<p>Write a dataframe either to a local folder or the production one</p> Source code in <code>ddataflow/ddataflow.py</code> <pre><code>def write(self, df, name: str):\n    \"\"\"\n    Write a dataframe either to a local folder or the production one\n    \"\"\"\n    if name not in self._data_writers:\n        raise WriterNotFoundException(name)\n\n    if self._ddataflow_enabled:\n        writing_path = self._snapshot_path\n\n        if self._offline_enabled:\n            writing_path = self._local_path\n        else:\n            if not writing_path.startswith(DDataflow._DEFAULT_SNAPSHOT_BASE_PATH):\n                raise Exception(\n                    f\"Only writing to {DDataflow._DEFAULT_SNAPSHOT_BASE_PATH} is enabled\"\n                )\n\n        writing_path = os.path.join(writing_path, name)\n        logger.info(\"Writing data to parquet file: \" + writing_path)\n        return df.write.parquet(writing_path, mode=\"overwrite\")\n\n    # if none of the above writes to production\n    return self._data_writers[name][\"writer\"](df, name, self._get_spark())  # type: ignore\n</code></pre>"},{"location":"api_reference/DataSource/","title":"DataSource","text":""},{"location":"api_reference/DataSource/#ddataflow.data_source.DataSource","title":"<code>DataSource</code>","text":"<p>Utility functions at data source level</p> Source code in <code>ddataflow/data_source.py</code> <pre><code>class DataSource:\n    \"\"\"\n    Utility functions at data source level\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        name: str,\n        config: dict,\n        local_data_folder: str,\n        snapshot_path: str,\n        size_limit,\n    ):\n        self._name = name\n        self._local_data_folder = local_data_folder\n        self._snapshot_path = snapshot_path\n        self._size_limit = size_limit\n        self._config = config\n        self._filter = None\n        self._source = None\n\n        if \"source\" in self._config:\n            self._source = config[\"source\"]\n        else:\n            if self._config.get(\"file-type\") == \"parquet\":\n                self._source = lambda spark: spark.read.parquet(self._name)\n            else:\n                self._source = lambda spark: spark.table(self._name)\n\n        if \"filter\" in self._config:\n            self._filter = self._config[\"filter\"]\n        else:\n            if self._config.get(\"default_sampling\"):\n                self._filter = lambda df: filter_function(df)\n\n    def query(self):\n        \"\"\"\n        query with filter unless none is present\n        \"\"\"\n        df = self.query_without_filter()\n\n        if self._filter is not None:\n            print(f\"Filter set for {self._name}, applying it\")\n            df = self._filter(df)\n        else:\n            print(f\"No filter set for {self._name}\")\n\n        return df\n\n    def has_filter(self) -&gt; bool:\n        return self._filter is not None\n\n    def query_without_filter(self):\n        \"\"\"\n        Go to the raw data source without any filtering\n        \"\"\"\n        spark = get_or_create_spark()\n        logger.debug(f\"Querying without filter source: '{self._name}'\")\n        return self._source(spark)\n\n    def query_locally(self):\n        logger.info(f\"Querying locally {self._name}\")\n\n        path = self.get_local_path()\n        if not os.path.exists(path):\n            raise Exception(\n                f\"\"\"Data source '{self.get_name()}' does not have data in {path}.\n            Consider downloading using  the following command:\n            ddataflow current_project download_data_sources\"\"\"\n            )\n        spark = get_or_create_spark()\n        df = spark.read.parquet(path)\n\n        return df\n\n    def get_dbfs_sample_path(self) -&gt; str:\n        return os.path.join(self._snapshot_path, self._get_name_as_path())\n\n    def get_local_path(self) -&gt; str:\n        return os.path.join(self._local_data_folder, self._get_name_as_path())\n\n    def _get_name_as_path(self):\n        \"\"\"\n        converts the name when it has \"/mnt/envents\" in the name to a single file in a (flat structure) _mnt_events\n        \"\"\"\n        return self.get_name().replace(\"/\", \"_\")\n\n    def get_name(self) -&gt; str:\n        return self._name\n\n    def get_parquet_filename(self) -&gt; str:\n        return self._name + \".parquet\"\n\n    def estimate_size_and_fail_if_too_big(self):\n        \"\"\"\n        Estimate the size of the data source use the _name used in the _config\n        It will throw an exception if the estimated size is bigger than the maximum allowed in the configuration\n        \"\"\"\n\n        print(\"Estimating size of data source: \", self.get_name())\n        df = self.query()\n        size_estimation = self._estimate_size(df)\n\n        print(\"Estimated size of the Dataset in GB: \", size_estimation)\n\n        if size_estimation &gt; self._size_limit:\n            raise BiggerThanMaxSize(self._name, size_estimation, self._size_limit)\n\n        return df\n\n    def _estimate_size(self, df: DataFrame) -&gt; float:\n        \"\"\"\n        Estimates the size of a dataframe in Gigabytes\n\n        Formula:\n            number of gigabytes = (N*V*W) / 1024^3\n        \"\"\"\n\n        print(f\"Amount of rows in dataframe to estimate size: {df.count()}\")\n        average_variable_size_bytes = 50\n        return (df.count() * len(df.columns) * average_variable_size_bytes) / (1024**3)\n</code></pre>"},{"location":"api_reference/DataSource/#ddataflow.data_source.DataSource.estimate_size_and_fail_if_too_big","title":"<code>estimate_size_and_fail_if_too_big()</code>","text":"<p>Estimate the size of the data source use the _name used in the _config It will throw an exception if the estimated size is bigger than the maximum allowed in the configuration</p> Source code in <code>ddataflow/data_source.py</code> <pre><code>def estimate_size_and_fail_if_too_big(self):\n    \"\"\"\n    Estimate the size of the data source use the _name used in the _config\n    It will throw an exception if the estimated size is bigger than the maximum allowed in the configuration\n    \"\"\"\n\n    print(\"Estimating size of data source: \", self.get_name())\n    df = self.query()\n    size_estimation = self._estimate_size(df)\n\n    print(\"Estimated size of the Dataset in GB: \", size_estimation)\n\n    if size_estimation &gt; self._size_limit:\n        raise BiggerThanMaxSize(self._name, size_estimation, self._size_limit)\n\n    return df\n</code></pre>"},{"location":"api_reference/DataSource/#ddataflow.data_source.DataSource.query","title":"<code>query()</code>","text":"<p>query with filter unless none is present</p> Source code in <code>ddataflow/data_source.py</code> <pre><code>def query(self):\n    \"\"\"\n    query with filter unless none is present\n    \"\"\"\n    df = self.query_without_filter()\n\n    if self._filter is not None:\n        print(f\"Filter set for {self._name}, applying it\")\n        df = self._filter(df)\n    else:\n        print(f\"No filter set for {self._name}\")\n\n    return df\n</code></pre>"},{"location":"api_reference/DataSource/#ddataflow.data_source.DataSource.query_without_filter","title":"<code>query_without_filter()</code>","text":"<p>Go to the raw data source without any filtering</p> Source code in <code>ddataflow/data_source.py</code> <pre><code>def query_without_filter(self):\n    \"\"\"\n    Go to the raw data source without any filtering\n    \"\"\"\n    spark = get_or_create_spark()\n    logger.debug(f\"Querying without filter source: '{self._name}'\")\n    return self._source(spark)\n</code></pre>"},{"location":"api_reference/DataSourceDownloader/","title":"DataSourceDownloader","text":""},{"location":"api_reference/DataSourceDownloader/#ddataflow.downloader.DataSourceDownloader","title":"<code>DataSourceDownloader</code>","text":"Source code in <code>ddataflow/downloader.py</code> <pre><code>class DataSourceDownloader:\n\n    _download_folder: str\n\n    def download_all(\n        self, data_sources: DataSources, overwrite: bool = True, debug=False\n    ):\n        \"\"\"\n        Download the data sources locally for development offline\n        Note: you need databricks-cli for this command to work\n\n        Options:\n            overwrite: will first clean the existing files\n        \"\"\"\n        self._download_folder = data_sources.download_folder\n        if overwrite:\n            if \".ddataflow\" not in self._download_folder:\n                raise Exception(\"Can only clean folders within .ddataflow\")\n\n            cmd_delete = f\"rm -rf {self._download_folder}\"\n            print(\"Deleting content from\", cmd_delete)\n            os.system(cmd_delete)\n\n        print(\"Starting to download the data-sources into your snapshot folder\")\n\n        for data_source_name in data_sources.all_data_sources_names():\n            print(f\"Starting download process for datasource: {data_source_name}\")\n            data_source = data_sources.get_data_source(data_source_name)\n            self._download_data_source(data_source, debug)\n\n        print(\"Download of all data-sources finished successfully!\")\n\n    def _download_data_source(self, data_source: DataSource, debug=False):\n        \"\"\"\n        Download the latest data snapshot to the local machine for developing locally\n        \"\"\"\n        os.makedirs(self._download_folder, exist_ok=True)\n\n        debug_str = \"\"\n        if debug:\n            debug_str = \"--debug\"\n\n        cmd = f'databricks fs cp {debug_str} -r \"{data_source.get_dbfs_sample_path()}\" \"{data_source.get_local_path()}\"'\n\n        logger.info(cmd)\n        result = os.system(cmd)\n\n        if result != 0:\n            raise Exception(\n                f\"\"\"\n            Databricks cli failed! See error message above.\n            Also consider rerunning the download command in your terminal to see the results.\n            {cmd}\n            \"\"\"\n            )\n</code></pre>"},{"location":"api_reference/DataSourceDownloader/#ddataflow.downloader.DataSourceDownloader.download_all","title":"<code>download_all(data_sources, overwrite=True, debug=False)</code>","text":"<p>Download the data sources locally for development offline Note: you need databricks-cli for this command to work</p> <p>Options:     overwrite: will first clean the existing files</p> Source code in <code>ddataflow/downloader.py</code> <pre><code>def download_all(\n    self, data_sources: DataSources, overwrite: bool = True, debug=False\n):\n    \"\"\"\n    Download the data sources locally for development offline\n    Note: you need databricks-cli for this command to work\n\n    Options:\n        overwrite: will first clean the existing files\n    \"\"\"\n    self._download_folder = data_sources.download_folder\n    if overwrite:\n        if \".ddataflow\" not in self._download_folder:\n            raise Exception(\"Can only clean folders within .ddataflow\")\n\n        cmd_delete = f\"rm -rf {self._download_folder}\"\n        print(\"Deleting content from\", cmd_delete)\n        os.system(cmd_delete)\n\n    print(\"Starting to download the data-sources into your snapshot folder\")\n\n    for data_source_name in data_sources.all_data_sources_names():\n        print(f\"Starting download process for datasource: {data_source_name}\")\n        data_source = data_sources.get_data_source(data_source_name)\n        self._download_data_source(data_source, debug)\n\n    print(\"Download of all data-sources finished successfully!\")\n</code></pre>"},{"location":"api_reference/DataSources/","title":"DataSources","text":""},{"location":"api_reference/DataSources/#ddataflow.data_sources.DataSources","title":"<code>DataSources</code>","text":"<p>Validates and Abstract the access to data sources</p> Source code in <code>ddataflow/data_sources.py</code> <pre><code>class DataSources:\n    \"\"\"\n    Validates and Abstract the access to data sources\n    \"\"\"\n\n    def __init__(\n        self, *, config, local_folder: str, snapshot_path: str, size_limit: int\n    ):\n        self.config = config\n        self.data_source: Dict[str, Any] = {}\n        self.download_folder = local_folder\n        for data_source_name, data_source_config in self.config.items():\n            self.data_source[data_source_name] = DataSource(\n                name=data_source_name,\n                config=data_source_config,\n                local_data_folder=local_folder,\n                snapshot_path=snapshot_path,\n                size_limit=size_limit,\n            )\n\n    def all_data_sources_names(self) -&gt; List[str]:\n        return list(self.data_source.keys())\n\n    def get_data_source(self, name) -&gt; DataSource:\n        if name not in self.data_source:\n            raise Exception(f\"Data source does not exist {name}\")\n        return self.data_source[name]\n\n    def get_filter(self, data_source_name: str):\n        return self.config[data_source_name][\"query\"]\n\n    def get_parquet_name(self, data_source_name: str):\n        return self.config[data_source_name][\"parquet_name\"]\n</code></pre>"}]}